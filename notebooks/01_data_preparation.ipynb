{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Preparation\n",
    "\n",
    "This notebook handles:\n",
    "1. Loading raw data files (enrolment, demographic updates, biometric updates)\n",
    "2. Validating schemas and checking data quality\n",
    "3. Merging datasets on (date, state, district)\n",
    "4. Calculating core derived metrics\n",
    "5. Saving processed data to parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src import config\n",
    "from src.preprocessing import (\n",
    "    load_enrolment,\n",
    "    load_demographic,\n",
    "    load_biometric,\n",
    "    merge_datasets,\n",
    "    calculate_core_metrics,\n",
    "    handle_missing_values,\n",
    "    save_processed,\n",
    "    validate_schema,\n",
    "    check_date_continuity\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"✓ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Enrolment Data\n",
    "print(\"Loading enrolment data...\")\n",
    "enrol_df = load_enrolment(config.ENROLMENT_FILE)\n",
    "\n",
    "print(f\"\\nEnrolment Dataset Summary:\")\n",
    "print(f\"  - Rows: {len(enrol_df):,}\")\n",
    "print(f\"  - Columns: {enrol_df.columns.tolist()}\")\n",
    "print(f\"  - Date range: {enrol_df['date'].min()} to {enrol_df['date'].max()}\")\n",
    "print(f\"  - States: {enrol_df['state'].nunique()}\")\n",
    "print(f\"  - Districts: {enrol_df['district'].nunique()}\")\n",
    "print(f\"  - Total enrolments: {enrol_df[config.METRIC_ENROLMENT_TOTAL].sum():,.0f}\")\n",
    "\n",
    "enrol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Demographic Update Data\n",
    "print(\"Loading demographic update data...\")\n",
    "demo_df = load_demographic(config.DEMOGRAPHIC_UPDATE_FILE)\n",
    "\n",
    "print(f\"\\nDemographic Update Dataset Summary:\")\n",
    "print(f\"  - Rows: {len(demo_df):,}\")\n",
    "print(f\"  - Columns: {demo_df.columns.tolist()}\")\n",
    "print(f\"  - Date range: {demo_df['date'].min()} to {demo_df['date'].max()}\")\n",
    "print(f\"  - States: {demo_df['state'].nunique()}\")\n",
    "print(f\"  - Districts: {demo_df['district'].nunique()}\")\n",
    "print(f\"  - Total updates: {demo_df[config.METRIC_DEMOGRAPHIC_UPDATES_TOTAL].sum():,.0f}\")\n",
    "\n",
    "demo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Biometric Update Data\n",
    "print(\"Loading biometric update data...\")\n",
    "bio_df = load_biometric(config.BIOMETRIC_UPDATE_FILE)\n",
    "\n",
    "print(f\"\\nBiometric Update Dataset Summary:\")\n",
    "print(f\"  - Rows: {len(bio_df):,}\")\n",
    "print(f\"  - Columns: {bio_df.columns.tolist()}\")\n",
    "print(f\"  - Date range: {bio_df['date'].min()} to {bio_df['date'].max()}\")\n",
    "print(f\"  - States: {bio_df['state'].nunique()}\")\n",
    "print(f\"  - Districts: {bio_df['district'].nunique()}\")\n",
    "print(f\"  - Total updates: {bio_df[config.METRIC_BIOMETRIC_UPDATES_TOTAL].sum():,.0f}\")\n",
    "\n",
    "bio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Schema Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"SCHEMA VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validate enrolment schema\n",
    "enrol_valid, enrol_errors = validate_schema(enrol_df, config.ENROLMENT_SCHEMA, 'enrolment')\n",
    "print(f\"\\nEnrolment: {'✓ PASSED' if enrol_valid else '✗ FAILED'}\")\n",
    "if not enrol_valid:\n",
    "    for err in enrol_errors:\n",
    "        print(f\"  - {err}\")\n",
    "\n",
    "# Validate demographic schema\n",
    "demo_valid, demo_errors = validate_schema(demo_df, config.DEMOGRAPHIC_SCHEMA, 'demographic')\n",
    "print(f\"\\nDemographic: {'✓ PASSED' if demo_valid else '✗ FAILED'}\")\n",
    "if not demo_valid:\n",
    "    for err in demo_errors:\n",
    "        print(f\"  - {err}\")\n",
    "\n",
    "# Validate biometric schema\n",
    "bio_valid, bio_errors = validate_schema(bio_df, config.BIOMETRIC_SCHEMA, 'biometric')\n",
    "print(f\"\\nBiometric: {'✓ PASSED' if bio_valid else '✗ FAILED'}\")\n",
    "if not bio_valid:\n",
    "    for err in bio_errors:\n",
    "        print(f\"  - {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Check Date Continuity\n",
    "print(\"=\" * 60)\n",
    "print(\"DATE CONTINUITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, df in [('Enrolment', enrol_df), ('Demographic', demo_df), ('Biometric', bio_df)]:\n",
    "    is_cont, gaps = check_date_continuity(df, group_cols=['state', 'district'])\n",
    "    print(f\"\\n{name}: {'✓ No significant gaps' if is_cont else f'✗ Found {len(gaps)} gaps'}\")\n",
    "    if not is_cont and len(gaps) > 0:\n",
    "        print(gaps.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Missing Values Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, df in [('Enrolment', enrol_df), ('Demographic', demo_df), ('Biometric', bio_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "    print(missing_df[missing_df['Missing'] > 0] if missing_df['Missing'].sum() > 0 else \"  No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Merge all datasets\n",
    "print(\"Merging datasets on (date, state, district)...\")\n",
    "merged_df = merge_datasets(enrol_df, demo_df, bio_df)\n",
    "\n",
    "print(f\"\\nMerged Dataset Summary:\")\n",
    "print(f\"  - Rows: {len(merged_df):,}\")\n",
    "print(f\"  - Columns: {len(merged_df.columns)}\")\n",
    "print(f\"  - States: {merged_df['state'].nunique()}\")\n",
    "print(f\"  - Districts: {merged_df['district'].nunique()}\")\n",
    "print(f\"  - Date range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "\n",
    "print(f\"\\nColumn list:\")\n",
    "print(merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Verify merge integrity\n",
    "print(\"=\" * 60)\n",
    "print(\"MERGE INTEGRITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for rows with only enrolment data\n",
    "enrol_only = merged_df[config.METRIC_ENROLMENT_TOTAL].notna() & merged_df[config.METRIC_DEMOGRAPHIC_UPDATES_TOTAL].isna()\n",
    "demo_only = merged_df[config.METRIC_DEMOGRAPHIC_UPDATES_TOTAL].notna() & merged_df[config.METRIC_ENROLMENT_TOTAL].isna()\n",
    "bio_only = merged_df[config.METRIC_BIOMETRIC_UPDATES_TOTAL].notna() & merged_df[config.METRIC_ENROLMENT_TOTAL].isna()\n",
    "\n",
    "print(f\"Rows with enrolment data only: {enrol_only.sum():,}\")\n",
    "print(f\"Rows with demographic data only: {demo_only.sum():,}\")\n",
    "print(f\"Rows with biometric data only: {bio_only.sum():,}\")\n",
    "print(f\"Rows with complete data: {(~enrol_only & ~demo_only & ~bio_only).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Core Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Calculate all derived metrics\n",
    "print(\"Calculating core metrics...\")\n",
    "df = calculate_core_metrics(merged_df)\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "new_cols = [c for c in df.columns if c not in merged_df.columns]\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nMetric Statistics:\")\n",
    "metric_cols = [\n",
    "    config.METRIC_ENROLMENT_TOTAL,\n",
    "    config.METRIC_TOTAL_UPDATES,\n",
    "    config.METRIC_UPDATE_TO_ENROLMENT_RATIO,\n",
    "    config.METRIC_ENROLMENT_VELOCITY,\n",
    "    config.METRIC_ENROLMENT_VOLATILITY\n",
    "]\n",
    "df[metric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Verify metric calculations\n",
    "print(\"=\" * 60)\n",
    "print(\"METRIC VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify total_updates = demographic + biometric\n",
    "expected_total = df[config.METRIC_DEMOGRAPHIC_UPDATES_TOTAL].fillna(0) + df[config.METRIC_BIOMETRIC_UPDATES_TOTAL].fillna(0)\n",
    "actual_total = df[config.METRIC_TOTAL_UPDATES]\n",
    "match = (expected_total == actual_total).all()\n",
    "print(f\"total_updates calculation: {'✓ CORRECT' if match else '✗ MISMATCH'}\")\n",
    "\n",
    "# Verify update_to_enrolment_ratio\n",
    "sample_idx = df[config.METRIC_ENROLMENT_TOTAL] > 0\n",
    "expected_ratio = df.loc[sample_idx, config.METRIC_TOTAL_UPDATES] / df.loc[sample_idx, config.METRIC_ENROLMENT_TOTAL]\n",
    "actual_ratio = df.loc[sample_idx, config.METRIC_UPDATE_TO_ENROLMENT_RATIO]\n",
    "ratio_close = np.allclose(expected_ratio, actual_ratio, rtol=1e-5)\n",
    "print(f\"update_to_enrolment_ratio calculation: {'✓ CORRECT' if ratio_close else '✗ MISMATCH'}\")\n",
    "\n",
    "# Check velocity and volatility have been calculated\n",
    "print(f\"enrolment_velocity calculated: {'✓' if df[config.METRIC_ENROLMENT_VELOCITY].notna().sum() > 0 else '✗'}\")\n",
    "print(f\"enrolment_volatility calculated: {'✓' if df[config.METRIC_ENROLMENT_VOLATILITY].notna().sum() > 0 else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Handle missing values\n",
    "print(f\"Before cleaning: {len(df):,} rows\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum():,}\")\n",
    "\n",
    "df = handle_missing_values(df)\n",
    "\n",
    "print(f\"\\nAfter cleaning: {len(df):,} rows\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Save processed data\n",
    "output_path = save_processed(df, config.MERGED_DATA_FILE)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✓ Processed {len(df):,} rows across {df['district'].nunique()} districts\")\n",
    "print(f\"✓ Saved to: {output_path}\")\n",
    "print(f\"\\nFile size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Final dataset preview\n",
    "print(\"Final Dataset Sample:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Dataset info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
